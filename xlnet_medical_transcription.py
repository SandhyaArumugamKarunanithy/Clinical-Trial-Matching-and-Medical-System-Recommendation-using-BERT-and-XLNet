# -*- coding: utf-8 -*-
"""xlnetmedicaltranscription.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p4JmU7QvVFf0EBQ-Ay2ivVuigKkoYPe8
"""

!pip install transformers
!pip install sentencepiece

import torch
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

from transformers import XLNetTokenizer, XLNetModel, AdamW
from transformers import XLNetForSequenceClassification, get_linear_schedule_with_warmup

from torch.utils.data import DataLoader, RandomSampler, TensorDataset

def find_available_GPU():
  if torch.cuda.is_available():
    print("GPU is available!")
    return torch.device('cuda')
  else:
    print("There is no GPU available at the moment")
    return torch.device('cpu')

def read_data(file_name):
  df = pd.read_csv(file_name)
  return df[['phrase','prompt']]


def split_data(df, t_size, tv_size):
  # test set
  remaining_x, x_test, remaining_y, y_test = train_test_split(df["phrase"], df["prompt"], 
                                          stratify = df["prompt"], 
                                          test_size = t_size, 
                                          random_state= 42)

  # train and validation set
  x_train, x_val, y_train, y_val = train_test_split(remaining_x, remaining_y, 
                                                    stratify = remaining_y, 
                                                    test_size = tv_size, 
                                                    random_state= 42)
  return x_train, y_train, x_val, y_val, x_test, y_test

def get_max_len_of_token(tokenizer, x_train):
  max_len = 0
  for sentence in x_train:
    # tokenize each sentence and add speical tokens
    input_ids = tokenizer.encode(sentence, add_special_tokens=True)
    # get the maximum sentence length
    if len(input_ids) > max_len:
      max_len = len(input_ids)

  return max_len

def get_tokenized_data(tokenizer, x_train, x_test, x_val, max_len):
  train = tokenizer(list(x_train), 
                    padding=True, 
                    truncation=True, 
                    max_length=max_len, 
                    return_tensors="pt") 
  test = tokenizer(list(x_test), 
                   padding=True, 
                   truncation=True, 
                   max_length=max_len, 
                   return_tensors="pt")
  val = tokenizer(list(x_val), 
                  padding=True, 
                  truncation=True, 
                  max_length=max_len, 
                  return_tensors="pt")
  return train, test, val

def encode_label_to_tensor(label_encoder, y_train, y_test, y_val):
  
  y_train_tensor = label_encoder.fit_transform(y_train)
  y_test_tensor = label_encoder.fit_transform(y_test)
  y_val_tensor = label_encoder.fit_transform(y_val)

  return torch.tensor(y_train_tensor), torch.tensor(y_test_tensor), torch.tensor(y_val_tensor)

def get_data_loaders(data_type, input_id, mask, label):
  
  tensor_dataset = TensorDataset(input_id, mask, label)
  rand_sampler = RandomSampler(tensor_dataset)

  if data_type == "train":
    dataloader = DataLoader(tensor_dataset, sampler=rand_sampler,batch_size=32)

  elif data_type == "validation" or "test":
    dataloader = DataLoader(tensor_dataset, sampler=rand_sampler,batch_size=64)
  
  else:
    print("invalid data_type")
    dataloader = 0
  
  return dataloader

device = find_available_GPU()

df = read_data("overview-of-recordings.csv")
x_train, y_train, x_val, y_val, x_test, y_test = split_data(df, 0.3, 0.1)

tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)

max_len = get_max_len_of_token(tokenizer, x_train)
tokenized_train, tokenized_test, tokenized_validation = get_tokenized_data(tokenizer, x_train, x_test, x_val, max_len)

train_id = tokenized_train['input_ids']
train_mask = tokenized_train['attention_mask']
test_id = tokenized_test['input_ids']
test_mask = tokenized_test['attention_mask']
val_id = tokenized_validation['input_ids']
val_mask = tokenized_validation['attention_mask']

label_encoder = LabelEncoder()
train_y, test_y, val_y = encode_label_to_tensor(label_encoder, y_train, y_test, y_val)

train_dataloader = get_data_loaders("train", train_id, train_mask, train_y)
test_dataloader = get_data_loaders("test", test_id, test_mask, test_y)
validation_dataloader = get_data_loaders("validation", val_id, val_mask, val_y)

class XLNet:
  def __init__(self, epoch, lr, eps, device):
    self.device = device
    self.epoch = epoch
    self.lr = lr
    self.eps = eps
    self.model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=25)
    self.model.cuda()
    self.optimizer = AdamW(self.model.parameters(), lr=self.lr, eps = self.eps)
    self.schedule = get_linear_schedule_with_warmup(self.optimizer, 
                                                    num_warmup_steps=0, 
                                                    num_training_steps=len(train_dataloader) * self.epoch)

  def train(self, train_dataloader, validation_loader):
    training_loss_list = []
    training_acc_list = []
    val_loss_list = []
    val_acc_list = []

    for e in range(0, self.epoch):
      print("epoch: ",(e+1))

      train_loss = 0 
      pred = 0
      data_num = 0
      self.model.train() # training mode


      for ba_da in train_dataloader:
        # print(ba_da[0])
        # print(ba_da[1])
        # print(ba_da[2])
        id, mask, class_label = ba_da

        class_label = class_label.to(self.device, dtype=torch.long)

        self.model.zero_grad() # clear prev gradients

        output = self.model(id.to(self.device, dtype=torch.long), 
                            mask.to(self.device, dtype=torch.long))
        # print(output)
      
        loss_fn = torch.nn.CrossEntropyLoss()
        loss = loss_fn(output[0], class_label)

        _, idx_label = torch.max(output[0], dim=1)
        pred += (idx_label == class_label).sum().item()
        data_num += class_label
        train_loss += loss.item()
     
        loss.backward()

        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
        self.optimizer.step()

        self.schedule.step()

      avg_loss = train_loss / len(train_dataloader)
      train_acc = pred / data_num.size(0)

      training_loss_list.append(avg_loss)
      training_acc_list.append(train_acc)

      '''
      validation
      '''
      self.model.eval() # evaluation mode
      val_loss = 0
      val_data = 0
      val_pred = 0

      with torch.no_grad():

        for d in validation_loader:
          v_ids, v_mask, v_class_label = d
          v_class_label = v_class_label.to(device, dtype=torch.long)

          v_outputs = self.model(v_ids.to(device, dtype=torch.long), 
                                v_mask.to(device, dtype=torch.long))

          v_loss_fn = torch.nn.CrossEntropyLoss()
          v_loss = v_loss_fn(v_outputs[0], v_class_label)

          _, v_idx_label = torch.max(v_outputs[0], dim=1)
          val_pred += (v_idx_label == v_class_label).sum().item()
          val_data += v_class_label
          val_loss += v_loss.item()

      val_avg_loss = val_loss / len(validation_loader)
      val_acc = val_pred / val_data.size(0)

      val_loss_list.append(val_avg_loss)
      val_acc_list.append(val_acc)

      print("val_loss:", val_avg_loss)
      print("val_acc: ", val_acc)
      print("train loss: ", avg_loss)
      print("train acc: ", train_acc)

    return training_loss_list, training_acc_list, val_loss_list, val_acc_list

  def test(self, test_loader):
    self.model.eval() # evaluation mode
    test_data = 0
    test_pred = 0

    for d in test_loader:
      with torch.no_grad():
        test_id, test_mask, test_class = d
        test_class = test_class.to(device, dtype=torch.long)

        test_output = self.model(test_id.to(device, dtype=torch.long), 
                                 test_mask.to(device, dtype=torch.long))
        
        _, test_idx = torch.max(test_output[0], dim=1)
        test_pred += (test_idx == test_class).sum().item()
        test_data += test_class

    test_acc = test_pred / test_data.size(0)

    return test_acc *100

EPOCH = 5
LR = 1e-5
EPS = 1e-08

XLNet_model = XLNet(EPOCH, LR, EPS, device)
training_loss_list, training_acc_list, val_loss_list, val_acc_list = XLNet_model.train(train_dataloader, validation_dataloader)

test_acc = XLNet_model.test(test_dataloader)

print(test_acc)

import matplotlib.pyplot as plt 

loss_df = pd.DataFrame(data={'Epoch':[1,2,3,4,5], 
                        'Training Loss': training_loss_list,
                        'Validation Loss': val_loss_list})

fig, ax = plt.subplots()
loss_df.plot(x = 'Epoch', y = 'Training Loss', ax = ax) 
loss_df.plot(x = 'Epoch', y = 'Validation Loss', ax = ax, secondary_y = True)

acc_df = pd.DataFrame(data={'Epoch':[1,2,3,4,5], 
                        'Training Accuracy': training_acc_list, 
                        'Validation Accuracy': val_acc_list})

fig1, ax = plt.subplots()
acc_df.plot(x = 'Epoch', y = 'Training Accuracy', ax = ax) 
acc_df.plot(x = 'Epoch', y = 'Validation Accuracy', ax = ax, secondary_y = True)